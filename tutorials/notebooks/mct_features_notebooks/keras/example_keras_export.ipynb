{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Export Quantized Keras Model\n",
        "\n",
        "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_export.ipynb)\n",
        "\n",
        "\n",
        "To export a TensorFlow model as a quantized model, it is necessary to first apply quantization\n",
        "to the model using MCT:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UJDzewEYfSN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_VER = '2.14.0'\n",
        "\n",
        "!pip install -q tensorflow=={TF_VER}\n",
        "! pip install -q mct-nightly"
      ],
      "metadata": {
        "id": "qNddNV6TEsX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.applications import MobileNetV2\n",
        "import model_compression_toolkit as mct\n",
        "\n",
        "# Create a model\n",
        "float_model = MobileNetV2()\n",
        "# Quantize the model.\n",
        "# Notice that here the representative dataset is random for demonstration only.\n",
        "quantized_exportable_model, _ = mct.ptq.keras_post_training_quantization(float_model,\n",
        "                                                                         representative_data_gen=lambda: [np.random.random((1, 224, 224, 3))])"
      ],
      "metadata": {
        "id": "eheBYKxRDFgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### keras\n",
        "\n",
        "The model will be exported as a tensorflow `.keras` model where weights and activations are quantized but represented using a float32 dtype.\n",
        "Two optional quantization formats are available: MCTQ and FAKELY_QUANT.\n",
        "\n",
        "#### MCTQ\n",
        "\n",
        "By default, `mct.exporter.keras_export_model` will export the quantized Keras model to\n",
        "a .keras model with custom quantizers from mct_quantizers module.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-n70LVe6DQPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of exported model\n",
        "keras_file_path = 'exported_model_mctq.keras'\n",
        "\n",
        "# Export a keras model with mctq custom quantizers.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=keras_file_path)"
      ],
      "metadata": {
        "id": "PO-Hh0bzD1VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the model has the same size as the quantized exportable model as weights data types are float.\n",
        "\n",
        "#### MCTQ - Loading Exported Model\n",
        "\n",
        "To load the exported model with MCTQ quantizers, use `mct.keras_load_quantized_model`:"
      ],
      "metadata": {
        "id": "Bwx5rxXDF_gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = mct.keras_load_quantized_model(keras_file_path)"
      ],
      "metadata": {
        "id": "q235XNJQmTdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Fakely-Quantized"
      ],
      "metadata": {
        "id": "sOmDjSehlQba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of exported model\n",
        "keras_file_path = 'exported_model_fakequant.keras'\n",
        "\n",
        "# Use mode KerasExportSerializationFormat.KERAS for a .keras model\n",
        "# and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights\n",
        "# and activations.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=keras_file_path,\n",
        "                                quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
      ],
      "metadata": {
        "id": "WLyHEEiwGByT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the fakely-quantized model has the same size as the quantized exportable model as weights data types are\n",
        "float.\n",
        "\n",
        "\n",
        "\n",
        "### TFLite\n",
        "The tflite serialization format export in two qauntization formats: INT8 and FAKELY_QUANT.\n",
        "\n",
        "#### INT8 TFLite\n",
        "\n",
        "The model will be exported as a tflite model where weights and activations are represented as 8bit integers."
      ],
      "metadata": {
        "id": "-L1aRxFGGFeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_file_path = 'exported_model_int8.tflite'\n",
        "\n",
        "# Use mode KerasExportSerializationFormat.TFLITE for tflite model and quantization_format.INT8.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=tflite_file_path,\n",
        "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
        "                                quantization_format=mct.exporter.QuantizationFormat.INT8)"
      ],
      "metadata": {
        "id": "V4I-p1q5GLzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare size of float and quantized model:\n"
      ],
      "metadata": {
        "id": "SBqtJV9AGRzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Save float model to measure its size\n",
        "float_file_path = 'exported_model_float.keras'\n",
        "float_model.save(float_file_path)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file_path) / float(2 ** 20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(tflite_file_path) / float(2 ** 20))"
      ],
      "metadata": {
        "id": "LInM16OMGUtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Fakely-Quantized TFLite\n",
        "\n",
        "The model will be exported as a tflite model where weights and activations are quantized but represented with a float data type.\n",
        "\n",
        "##### Usage Example"
      ],
      "metadata": {
        "id": "9eVDoIHiGX5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of exported model\n",
        "tflite_file_path = 'exported_model_fakequant.tflite'\n",
        "\n",
        "\n",
        "# Use mode KerasExportSerializationFormat.TFLITE for tflite model and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights\n",
        "# and activations.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=tflite_file_path,\n",
        "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
        "                                quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
      ],
      "metadata": {
        "id": "0OYLAbI8Gawu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Notice that the fakely-quantized model has the same size as the quantized exportable model as weights data types are\n",
        "float.\n"
      ],
      "metadata": {
        "id": "voOrtCroD-HE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7e1572"
      },
      "source": [
        "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    }
  ]
}
