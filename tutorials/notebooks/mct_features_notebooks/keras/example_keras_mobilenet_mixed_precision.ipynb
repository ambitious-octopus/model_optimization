{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# Post Training Mixed Precision Quantization using the Model Compression Toolkit - A Quick-Start Guide\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_mobilenet_mixed_precision.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates a pre-trained model quantization using the **Model Compression Toolkit (MCT)** with **Mixed Precision**. \n",
    "\n",
    "Mixed Precision enables quantization of different layers with different bit-width precisions, to fit the model into a set of hardware restrictions. \n",
    "\n",
    "As we will see, mixed-precision quantization is a simple yet effective quantization scheme for compressing a model to a desired model size.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial we will cover:\n",
    "\n",
    "1. Post-Training Mixed-Precision Quantization using MCT.\n",
    "2. Loading and preprocessing ImageNet's validation dataset.\n",
    "3. Constructing an unlabeled representative dataset.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install and import the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "\n",
    "!pip install -q tensorflow=={TF_VER}\n",
    "!pip install -q mct-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import model_compression_toolkit as mct\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b",
   "metadata": {
    "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization and mixed precision routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18b26e293b085e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !mv ILSVRC2012_devkit_t12.tar.gz imagenet/\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    !mv ILSVRC2012_img_val.tar imagenet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80cfeeced5e284",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aba047df87db08",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "if not os.path.isdir('imagenet/val'):\n",
    "    torchvision.datasets.ImageNet(root='./imagenet', split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028112db-3143-4fcb-96ae-e639e6476c31",
   "metadata": {
    "id": "028112db-3143-4fcb-96ae-e639e6476c31"
   },
   "source": [
    "Define the required preprocessing method for the pretrained model,\n",
    "and create a generator for the representative dataset, which is required for mixed precision quantization.\n",
    "\n",
    "The representative dataset is used for collecting statistics on the inference outputs of all layers in the model.\n",
    " \n",
    "In order to decide on the size of the representative dataset, we configure the batch size and the number of calibration iterations.\n",
    "This gives us the total number of samples that will be used during PTQ (batch_size x n_iter).\n",
    "In this example we set `batch_size = 50` and `n_iter = 10`, resulting in a total of 500 representative images.\n",
    "\n",
    "Please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e859077a736f4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imagenet_preprocess_input(images, labels):\n",
    "    \"\"\"\n",
    "    Use the keras applications preprocess function.\n",
    "    Args:\n",
    "        images: input image batch.\n",
    "        labels: input label batch.\n",
    "    Returns:\n",
    "        preprocessed images & labels\n",
    "    \"\"\"\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408f624-ab68-4989-95f8-f9d327882840",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_representative_dataset(n_iter=10, batch_size=50):\n",
    "    \"\"\"\n",
    "    Download the ImageNet validation set locally and create the representative dataset generator.\n",
    "    Returns:\n",
    "        representative dataset generator for calibration\n",
    "    \"\"\"\n",
    "    print('loading dataset, this may take a few minutes ...')\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "\n",
    "    def representative_dataset():\n",
    "        for _ in range(n_iter):\n",
    "            yield [dataset.take(1).get_single_element()[0].numpy()]\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "representative_dataset_gen = get_representative_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Post-Training Mixed Precision quantization using MCT\n",
    "\n",
    "This is the main part in which we quantize our model.\n",
    "\n",
    "First, we load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cac59f-ec5e-41ca-b673-96220924a47c",
   "metadata": {
    "id": "80cac59f-ec5e-41ca-b673-96220924a47c"
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "float_model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9",
   "metadata": {
    "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9"
   },
   "source": [
    "Next, we need to define a **mixed precision quantization configuration** with possible mixed precision search options.\n",
    "MCT will search a mixed precision solution (namely, bit-width assignment for each layer)\n",
    "and quantize the model according to this configuration.\n",
    "**Note** that you can skip this part if you prefer to use the default quantization settings.\n",
    "\n",
    "In addition, we need to define a `TargetPlatformCapability` object, representing the HW specifications on which we wish to eventually deploy our quantized model.\n",
    "The candidates bit-width for quantization are defined in the target platform model. \n",
    "\n",
    "Finally, we need to set the **hardware constraints** which we want our quantized model to fit into.\n",
    "These are defined using a `ResourceUtilization` object.\n",
    "In this example, we set a **weights memory** constraint, by computing the size of the desired model's parameters under a compression of the model to 75% of its fixed-point 8-bit precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edacb5b7779e4d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enable Mixed-Precision config. For the sake of running faster, the hessian-based scores are disabled in this tutorial\n",
    "mp_config = mct.core.MixedPrecisionQuantizationConfig(\n",
    "    num_of_images=32,\n",
    "    use_hessian_based_scores=False)\n",
    "core_config = mct.core.CoreConfig(mixed_precision_config=mp_config)\n",
    "# Specify the target platform capability (TPC)\n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
    "\n",
    "# Get Resource Utilization information to constraint your model's memory size. Retrieve a ResourceUtilization object with helpful information of each resource metric, to constraint the quantized model to the desired memory size.\n",
    "resource_utilization_data = mct.core.keras_resource_utilization_data(float_model,\n",
    "                                   representative_dataset_gen,\n",
    "                                   core_config=core_config,\n",
    "                                   target_platform_capabilities=tpc)\n",
    "\n",
    "# Set a constraint for each of the Resource Utilization metrics.\n",
    "# Create a ResourceUtilization object to limit our returned model's size. Note that this values affects only layers and attributes\n",
    "# that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,\n",
    "# while the bias will not)\n",
    "# examples:\n",
    "weights_compression_ratio = 0.75  # About 0.75 of the model's weights memory size when quantized with 8 bits.\n",
    "resource_utilization = mct.core.ResourceUtilization(resource_utilization_data.weights_memory * weights_compression_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162dd6dd1fce7ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Run model Post-Training Quantization\n",
    "Finally, we quantize our model using MCT's post-training quantization API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8373a-82a5-4b97-9a10-25ee2341d148",
   "metadata": {
    "id": "33f8373a-82a5-4b97-9a10-25ee2341d148"
   },
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "    float_model,\n",
    "    representative_dataset_gen,\n",
    "    target_resource_utilization=resource_utilization,\n",
    "    core_config=core_config,\n",
    "    target_platform_capabilities=tpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382ada6-d001-4564-907d-767fa4e9ec56",
   "metadata": {
    "id": "7382ada6-d001-4564-907d-767fa4e9ec56"
   },
   "source": [
    "That's it! Our model is now quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b",
   "metadata": {
    "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b"
   },
   "source": [
    "## Models evaluation\n",
    "\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, let's assume we downloaded the ImageNet validation dataset to a folder with the path below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c875-c4fc-4819-97e5-721805cba546",
   "metadata": {
    "id": "eef7c875-c4fc-4819-97e5-721805cba546",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_validation_dataset():\n",
    "    \"\"\"\n",
    "    Generate validation dataset\n",
    "    Returns:\n",
    "         the validation dataset\n",
    "    \"\"\"\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=50,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=False,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "    return dataset\n",
    "\n",
    "evaluation_dataset = get_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889d217-90a6-4615-8569-38dc9cdd5999",
   "metadata": {
    "id": "9889d217-90a6-4615-8569-38dc9cdd5999"
   },
   "source": [
    "Let's start with the floating-point model evaluation.\n",
    "\n",
    "We need to compile the model before evaluation and set the loss and the evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a0ae9-beaa-4af8-8481-49d4917c2209",
   "metadata": {
    "id": "1d3a0ae9-beaa-4af8-8481-49d4917c2209"
   },
   "outputs": [],
   "source": [
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "results = float_model.evaluate(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4a6f3-86a0-4e6c-8229-a2ff514f7b8c",
   "metadata": {
    "id": "ead4a6f3-86a0-4e6c-8229-a2ff514f7b8c"
   },
   "source": [
    "Finally, let's evaluate the quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc377ee-39b4-4ced-95db-f7d51ab60848",
   "metadata": {
    "id": "1bc377ee-39b4-4ced-95db-f7d51ab60848"
   },
   "outputs": [],
   "source": [
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "results = quantized_model.evaluate(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316c34cadd054e7",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfbb4de-5b6e-4732-83d3-a21e96cdd866",
   "metadata": {
    "id": "ebfbb4de-5b6e-4732-83d3-a21e96cdd866"
   },
   "source": [
    "You can see that we got a very small degradation with a compression rate of x4 !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6YjIdiRRjgkL",
   "metadata": {
    "id": "6YjIdiRRjgkL"
   },
   "source": [
    "Now, we can export the model to Keras and TFLite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z3CA16-ojoFL",
   "metadata": {
    "id": "z3CA16-ojoFL"
   },
   "outputs": [],
   "source": [
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path='qmodel.tflite',\n",
    "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
    "                                quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)\n",
    "\n",
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path='qmodel.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {
    "id": "14877777"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCT with mixed-precision with a few lines of code. We saw that we can achieve more than x4 compression ratio with minimal performance degradation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "\n",
    "\n",
    "Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
