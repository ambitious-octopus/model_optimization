{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Network Editor Usage\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_network_editor.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we will demonstrate how to leverage the Model Compression Toolkit (MCT) to quantize a simple Keras model and modify the quantization configuration for specific layers using the MCT's network editor. Our example model consists of a Conv2D layer followed by a Dense layer. Initially, we will quantize this model with a default configuration and inspect the bit allocation for each layer's weights. Then, we will introduce an edit rule to specifically quantize the Conv2D layer with a different bit width, showcasing the flexibility of MCT in customizing quantization schemes per layer.\n",
    "\n",
    "First, we install MCT and import requiered modules:"
   ],
   "metadata": {
    "id": "C_BBKEpTRqp_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6LXeaQD1c-w"
   },
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "\n",
    "!pip install -q tensorflow=={TF_VER}\n",
    "! pip install -q mct-nightly"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import model_compression_toolkit as mct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense\n",
    "from tensorflow.keras.models import Model"
   ],
   "metadata": {
    "id": "vCsjoKb7168U"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we create a simple Keras model with a Conv2D layer and a Dense layer:"
   ],
   "metadata": {
    "id": "bRPoKI-WSQn2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_shape = (16, 16, 3)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(filters=1, kernel_size=(3, 3))(inputs)\n",
    "x = Dense(units=10)(x)\n",
    "model = Model(inputs=inputs, outputs=x)"
   ],
   "metadata": {
    "id": "uOu8c7n_6Vd4"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial, for demonstration purposes and to expedite the process, we create a simple representative dataset generator using random data. This generator produces a batch of random input data matching the model's input shape."
   ],
   "metadata": {
    "id": "rDAMPxKhSYfx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 1\n",
    "def representative_data_gen():\n",
    "    yield [np.random.randn(batch_size, *input_shape)]\n"
   ],
   "metadata": {
    "id": "LvnQmku02qIM"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a function that takes a Keras model, a representative data generator, and a core configuration for quantization. The function utilizes Model Compression Toolkit's post-training quantization API:"
   ],
   "metadata": {
    "id": "VecsI-kDe9RM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def quantize_keras_mct(model, representative_data_gen, core_config):\n",
    "  quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "      in_model=model,\n",
    "      representative_data_gen=representative_data_gen,\n",
    "      core_config=core_config\n",
    "  )\n",
    "  return quantized_model\n"
   ],
   "metadata": {
    "id": "uIyyoMv93Bt7"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we start by setting a default core configuration for quantization using Model Compression Toolkit's CoreConfig. After quantizing the model with this configuration, we examine the number of bits used in the quantization of specific layers. We retrieve and print the number of bits used for the the layers' attribute called 'kernel' in both the Conv2D layer and the Dense layer. By default 8-bit are used for quantization across different types of layers in a model."
   ],
   "metadata": {
    "id": "Xqmg7vWNgsqc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Use default core config for observing baseline quantized model\n",
    "core_config = mct.core.CoreConfig()\n",
    "\n",
    "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config)\n",
    "conv2d_layer = quantized_model.layers[2]\n",
    "conv2d_nbits = conv2d_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
    "\n",
    "dense_layer = quantized_model.layers[4]\n",
    "dense_nbits = dense_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
    "\n",
    "print(f\"Conv2D nbits: {conv2d_nbits}, Dense nbits: {dense_nbits}\")"
   ],
   "metadata": {
    "id": "Z5VDv6Bz4cqN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Edit Configration Using Edit Rules List\n",
    "\n",
    " Now let's see how to customize the quantization process for specific layers using MCT's network editor. An `EditRule` is created with a `NodeTypeFilter` targeting the Conv2D layer type.\n",
    "\n",
    "  The action associated with this rule changes the quantization configuration of the 'kernel' attribute to 4 bits instead of the default 8 bits. This rule is then included in a list (`edit_rules_list`) which is passed to the `DebugConfig`.\n",
    "   \n",
    " The `DebugConfig`, with our custom rule, is then used to create a `CoreConfig`. This configuration will be applied when quantizing the model, resulting in the Conv2D layers being quantized using 4 bits while other layers follow the default setting."
   ],
   "metadata": {
    "id": "FyBwtQuMhQMt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "edit_rules_list = [\n",
    "    mct.core.network_editor.EditRule(\n",
    "        filter=mct.core.network_editor.NodeTypeFilter(Conv2D),\n",
    "        action=mct.core.network_editor.ChangeCandidatesWeightsQuantConfigAttr(attr_name='kernel', weights_n_bits=4)\n",
    "    )\n",
    "]\n",
    "\n",
    "debug_config = mct.core.DebugConfig(network_editor=edit_rules_list)\n",
    "core_config = mct.core.CoreConfig(debug_config=debug_config)"
   ],
   "metadata": {
    "id": "7YynVSSh3Mk-"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this final part of the tutorial, we apply the customized quantization process to our Keras model.\n",
    "\n",
    "By calling `quantize_keras_mct` with the `core_config` containing our edit rule, we specifically quantize the Conv2D layer using 4 bits, as per our custom configuration.\n",
    "\n",
    "The `quantized_model` now reflects these changes. We then extract and display the number of bits used for quantization in both the Conv2D and Dense layers.\n",
    "\n",
    "The output demonstrates the effect of our edit rule: the Conv2D layer is quantized with 4 bits while the Dense layer retains the default 8-bit quantization."
   ],
   "metadata": {
    "id": "ftkeDjZPiahd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config)\n",
    "conv2d_layer = quantized_model.layers[2]\n",
    "conv2d_nbits = conv2d_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
    "\n",
    "dense_layer = quantized_model.layers[4]\n",
    "dense_nbits = dense_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
    "\n",
    "print(f\"Conv2D nbits: {conv2d_nbits}, Dense nbits: {dense_nbits}\")"
   ],
   "metadata": {
    "id": "7p6qFWoEQBS5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Edit Z-Threshold for Activation Quantization\n",
    "\n",
    "In the context of model quantization, the Z-Threshold helps in handling outliers in the activation data. Outliers in the data can hurt the quantization process, leading to less efficient and potentially less accurate models.\n",
    "\n",
    "The Z-Threshold is used to set a boundary, beyond which extreme values in the activation data are considered outliers and are not used to determine the quantization parameters. This approach effectively filters out extreme values, ensuring a more robust and representative quantization.\n",
    "\n",
    "Adjusting the Z-Threshold can be particularly useful during the debugging and optimization of model quantization. By tweaking this parameter, you can fine-tune the balance between model accuracy and robustness against outliers in your specific use case.\n",
    "\n",
    "A higher Z-Threshold means more data is considered during quantization, including some outliers, which might be necessary for certain models or datasets.\n",
    "\n",
    "The following code demonstrates how you can customize the Z-Threshold for a specific layer type (Conv2D) in a Keras model using MCT's network editor functionality. This feature allows you to set different Z-Threshold values for different layers. By default, all layers use threshold of infinity (thus, no outlier-removal occurs)."
   ],
   "metadata": {
    "id": "2TqXTB48jKHx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "z_threshold_target = 5\n",
    "edit_rules_list = [\n",
    "    mct.core.network_editor.EditRule(\n",
    "        filter=mct.core.network_editor.NodeTypeFilter(Conv2D),\n",
    "        action=mct.core.network_editor.ChangeCandidatesActivationQuantConfigAttr(z_threshold=z_threshold_target)\n",
    "    )\n",
    "]\n",
    "\n",
    "debug_config = mct.core.DebugConfig(network_editor=edit_rules_list)\n",
    "core_config = mct.core.CoreConfig(debug_config=debug_config)\n",
    "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config)"
   ],
   "metadata": {
    "id": "VBRfQqZVjN3J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ],
   "metadata": {
    "id": "A1rhMoGUji1e"
   }
  }
 ]
}
