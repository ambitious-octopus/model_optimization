{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# Activation Threshold Search Demonstration For Post-Training Quantization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be59ea8-e208-4b64-aede-1dd6270b3540",
   "metadata": {
    "id": "9be59ea8-e208-4b64-aede-1dd6270b3540"
   },
   "source": [
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_activation_threshold_search.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9",
   "metadata": {
    "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762",
   "metadata": {
    "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762"
   },
   "source": [
    "This tutorial demonstrates the process used to find the activation threshold, a step that MCT uses during post-training quantization.\n",
    "\n",
    "In this example we will explore 2 metrics for threshold selection. We will start by demonstrating how to apply the corresponding MCT configurations, then, we will feed a representative dataset through the model, plot the activation distribution of two layers with their respective MCT calculated thresholds, and finally compare the quantized model accuracy of the two methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1",
   "metadata": {
    "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1"
   },
   "source": [
    "## Activation threshold explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a17f4-30c9-4caf-a888-424f7a82fbc8",
   "metadata": {
    "id": "a89a17f4-30c9-4caf-a888-424f7a82fbc8"
   },
   "source": [
    "During quantization process, thresholds are used to map a distribution of 32bit float values to their quantized counterparts. Doing this with the least loss of data while maintaining the most representative range is important for final model accuracy.\n",
    "\n",
    "How itâ€™s done in MCT?\n",
    "\n",
    "MCT's Post-training quantization uses a representative dataset to evaluate a list of typical output activation values. The challenge comes with how best to match these values to their quantized counterparts. To this end, a grid search for the optimal threshold is performed according to number of possible error metrics. Typically, mean squared error is the best performing metric and used by default.\n",
    "\n",
    "The error is calculated based on the difference between the float and quantized distribution. The threshold is selected based on the minimum error. For the case of MSE;\n",
    "\n",
    "$$\n",
    "ERR(t) = \\frac{1}{n_s} \\sum_{X \\in Fl(D)} (Q(X, t, n_b) - X)^2\n",
    "$$\n",
    "\n",
    "- $ERR(t)$ : The quantization error function dependent on threshold t.\n",
    "ns: The size of the representative dataset, indicating normalization over the dataset's size.\n",
    "\n",
    "- $\\sum$: Summation over all elements X in the flattened dataset $Fl(D)$.\n",
    "\n",
    "- $F_l(D)$: The collection of activation tensors in the l-th layer, representing the dataset D flattened for processing.\n",
    "\n",
    "- $Q(X, t, n_b)$: The quantized approximation of X, given a threshold t and bit width nb.\n",
    "\n",
    "- $X$: The original activation tensor before quantization.\n",
    "\n",
    "- $t$: The quantization threshold, a critical parameter for controlling the quantization process.\n",
    "\n",
    "- $n_b$: The number of bits used in the quantization process, affecting the model's precision and size.\n",
    "\n",
    "\n",
    "The quantization thresholds often have limitations, typically for deployment purposes. In MCT, activation thresholds are restricted by default to **Power of Two** values only and can represent signed values within the range of (-T, T) or unsigned values within the range of (0, T). Other restriction settings are available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e9543-d356-412f-acf1-c2ecad553e06",
   "metadata": {
    "id": "9c0e9543-d356-412f-acf1-c2ecad553e06"
   },
   "source": [
    "### Error methods supported by MCT:\n",
    "\n",
    "- NOCLIPPING - Use min/max values as thresholds.\n",
    "\n",
    "- MSE - Use min square error for minimizing quantizationnoises.\n",
    "\n",
    "- MAE - Use min absolute error for minimizing quantization nose.\n",
    "\n",
    "- KL - Use KL-divergen ce tosgnals disb as tas o be similar as posible.\n",
    "\n",
    "- Lp - Use Lpsingimizing quantization noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0",
   "metadata": {
    "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0",
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d",
   "metadata": {
    "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d"
   },
   "source": [
    "Install and import the relevant packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "\n",
    "!pip install -q tensorflow=={TF_VER}\n",
    "!pip install -q mct-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import model_compression_toolkit as mct\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z8F-avk3azgZ",
   "metadata": {
    "id": "z8F-avk3azgZ"
   },
   "source": [
    "Clone MCT to gain access to tutorial scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b675cf-e1b5-4249-a581-ffb9b1c16ba1",
   "metadata": {
    "id": "e3b675cf-e1b5-4249-a581-ffb9b1c16ba1"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sony/model_optimization.git local_mct\n",
    "!pip install -r ./local_mct/requirements.txt\n",
    "import sys\n",
    "sys.path.insert(0,\"./local_mct\")\n",
    "import tutorials.resources.utils.keras_tutorial_tools as tutorial_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b",
   "metadata": {
    "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecde59e4c37b1da",
   "metadata": {
    "collapsed": false,
    "id": "aecde59e4c37b1da"
   },
   "source": [
    "Load ImageNet classification dataset and seperate a small representative subsection of this dataset to use for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_ztv72uM6-UT",
   "metadata": {
    "id": "_ztv72uM6-UT"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !mv ILSVRC2012_devkit_t12.tar.gz imagenet/\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    !mv ILSVRC2012_img_val.tar imagenet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YVAoUjK47Zcp",
   "metadata": {
    "id": "YVAoUjK47Zcp"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "if not os.path.isdir('imagenet/val'):\n",
    "    ds = torchvision.datasets.ImageNet(root='./imagenet', split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb3eecae5346a9",
   "metadata": {
    "collapsed": false,
    "id": "fcbb3eecae5346a9"
   },
   "source": [
    "Here we create the representative dataset. For detail on this step see [ImageNet tutorial](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/imx500_notebooks/keras/example_keras_mobilenetv2_for_imx500.ipynb). If you are running locally a higher fraction of the dataset can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9ad33-f88c-4178-8f19-bac6b2b2e97b",
   "metadata": {
    "id": "eda9ad33-f88c-4178-8f19-bac6b2b2e97b"
   },
   "outputs": [],
   "source": [
    "REPRESENTATIVE_DATASET_FOLDER = './imagenet/val'\n",
    "BATCH_SIZE = 20\n",
    "fraction =0.001\n",
    "model_version = 'MobileNetV2'\n",
    "\n",
    "preprocessor = tutorial_tools.DatasetPreprocessor(model_version=model_version)\n",
    "representative_dataset_gen = preprocessor.get_representative_dataset(fraction, REPRESENTATIVE_DATASET_FOLDER, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
   },
   "source": [
    "## MCT Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615",
   "metadata": {
    "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615"
   },
   "source": [
    "This step we load the model and quantize with two methods of threshold error calculation: no clipping and MSE.\n",
    "\n",
    "No clipping chooses the lowest Power of two threshold that does not loose any data to its threshold.\n",
    "\n",
    "MSE chooses a Power of two threshold that results in the least difference between the float distribution and the quantized distribution.\n",
    "\n",
    "This means no clipping will often result in a larger threshold, which we will see later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VMrcPUN6jPlB",
   "metadata": {
    "id": "VMrcPUN6jPlB"
   },
   "source": [
    "First we load mobilenetv2 from the keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431848f-a5f4-4737-a5c8-f046a8bca840",
   "metadata": {
    "id": "c431848f-a5f4-4737-a5c8-f046a8bca840"
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "float_model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pd8blHyKjWay",
   "metadata": {
    "id": "Pd8blHyKjWay"
   },
   "source": [
    "Quantization perameters are defined. Here we will use default values apart from quantisation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca971297-e00b-44b5-b9e1-e57ba5843e38",
   "metadata": {
    "id": "ca971297-e00b-44b5-b9e1-e57ba5843e38"
   },
   "outputs": [],
   "source": [
    "from model_compression_toolkit import QuantizationErrorMethod\n",
    "\n",
    "# Specify the IMX500-v1 target platform capability (TPC)\n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
    "\n",
    "# List of error methods to iterate over\n",
    "q_configs_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vot-MCiWjzCE",
   "metadata": {
    "id": "Vot-MCiWjzCE"
   },
   "source": [
    "You can edit the code below to quantize with other error metrics MCT supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jtiZzXmTjxuI",
   "metadata": {
    "id": "jtiZzXmTjxuI"
   },
   "outputs": [],
   "source": [
    "# Error methods to iterate over\n",
    "error_methods = [\n",
    "    QuantizationErrorMethod.MSE,\n",
    "    QuantizationErrorMethod.NOCLIPPING\n",
    "]\n",
    "\n",
    "# If you are curious you can add any of the below quantization methods as well.\n",
    "#QuantizationErrorMethod.MAE\n",
    "#QuantizationErrorMethod.KL\n",
    "#QuantizationErrorMethod.LP\n",
    "\n",
    "# Iterate and build the QuantizationConfig objects\n",
    "for error_method in error_methods:\n",
    "    q_config = mct.QuantizationConfig(\n",
    "        activation_error_method=error_method,\n",
    "    )\n",
    "\n",
    "    q_configs_dict[error_method] = q_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8W3Dcn0jkJOH",
   "metadata": {
    "id": "8W3Dcn0jkJOH"
   },
   "source": [
    "Finally we quantize the model, this can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c6e55-d474-4dc3-9a43-44b736635998",
   "metadata": {
    "id": "ba0c6e55-d474-4dc3-9a43-44b736635998"
   },
   "outputs": [],
   "source": [
    "quantized_models_dict = {}\n",
    "\n",
    "for error_method, q_config in q_configs_dict.items():\n",
    "    # Create a CoreConfig object with the current quantization configuration\n",
    "    ptq_config = mct.core.CoreConfig(quantization_config=q_config)\n",
    "\n",
    "    # Perform MCT post-training quantization\n",
    "    quantized_model, quantization_info = mct.ptq.keras_post_training_quantization_experimental(\n",
    "        in_model=float_model,\n",
    "        representative_data_gen=representative_dataset_gen,\n",
    "        core_config=ptq_config,\n",
    "        target_platform_capabilities=tpc\n",
    "    )\n",
    "\n",
    "    # Update the dictionary to include the quantized model\n",
    "    quantized_models_dict[error_method] = {\n",
    "        \"quantization_config\": q_config,\n",
    "        \"quantized_model\": quantized_model,\n",
    "        \"quantization_info\": quantization_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8UHRsh2khM4",
   "metadata": {
    "id": "A8UHRsh2khM4"
   },
   "source": [
    "## Threshold and Distribution Visulisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y-0QLWFJkpFV",
   "metadata": {
    "id": "Y-0QLWFJkpFV"
   },
   "source": [
    "To assist with understanding we will now plot for two of Mobilenet's layers. The thresholds found during quantisation for both MSE error and NoClip, along side each layers activation distribution obtained by feeding the representative dataset through the model. This is useful to help visulise the effect of different thresholds on dataloss vs data resolution during quantisation. \n",
    "\n",
    "MCT quantization_info stores threshold data per layer. However, to see the distribution of the activations the model needs to be rebuilt upto and including the layer chosen for distribution visulisation.\n",
    "\n",
    "To do this we first need to list the layer names. With keras this can be done easily for the first 10 layes with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac",
   "metadata": {
    "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac"
   },
   "outputs": [],
   "source": [
    "for index, layer in enumerate(float_model.layers):\n",
    "    if index < 10:\n",
    "        print(layer.name)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277",
   "metadata": {
    "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277"
   },
   "source": [
    "First activation layer in model is 'Conv1_relu'.\n",
    "\n",
    "For this particular model, through testing we found that expanded_conv_project_BN shows differing thresholds for the two error metrics. So, this layer will also be visulised. For some context, MobileNetv2 uses an inverted residual structure where the input is expanded in the channel dimension, passed through a depthwise conv, and finally projected back to to a lower dimension. expanded_conv_project_BN layer represents this projection and the BN indicates Batch Normalisation.\n",
    "\n",
    "Use these layer names to create a pair of models that end in these respective layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d",
   "metadata": {
    "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "layer_name1 = 'Conv1_relu'\n",
    "layer_name2 = 'expanded_conv_project_BN'\n",
    "\n",
    "layer_output1 = float_model.get_layer(layer_name1).output\n",
    "activation_model_relu = Model(inputs=float_model.input, outputs=layer_output1)\n",
    "layer_output2 = float_model.get_layer(layer_name2).output\n",
    "activation_model_project = Model(inputs=float_model.input, outputs=layer_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364",
   "metadata": {
    "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364"
   },
   "source": [
    "Feed the representative dataset through these models and store the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb9888-5d67-4979-af50-80781a811b4b",
   "metadata": {
    "id": "eaeb9888-5d67-4979-af50-80781a811b4b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "activation_batches_relu = []\n",
    "activation_batches_project = []\n",
    "for images in representative_dataset_gen():\n",
    "    activations_relu = activation_model_relu.predict(images)\n",
    "    activation_batches_relu.append(activations_relu)\n",
    "    activations_project = activation_model_project.predict(images)\n",
    "    activation_batches_project.append(activations_project)\n",
    "\n",
    "all_activations_relu = np.concatenate(activation_batches_relu, axis=0).flatten()\n",
    "all_activations_project = np.concatenate(activation_batches_project, axis=0).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I5W9yY5DvOFr",
   "metadata": {
    "id": "I5W9yY5DvOFr"
   },
   "source": [
    "Thresholds calculated by MCT during quantization can be accessed using the following. The layer number matches the index of the layers named in the previous steps.\n",
    "\n",
    "As mentioned above we use the first activation relu layer and the batch normalisation layer as they best demonstrate the effect of the two threshold error methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NGnjrPD_uTd5",
   "metadata": {
    "id": "NGnjrPD_uTd5"
   },
   "outputs": [],
   "source": [
    "# layer 4 is the first activation layer - Conv1_relu\n",
    "layer_name2 = 'expanded_conv_project_BN'\n",
    "optimal_thresholds_relu = {\n",
    "    error_method: data[\"quantized_model\"].layers[4].activation_holder_quantizer.get_config()['threshold'][0] \n",
    "    for error_method, data in quantized_models_dict.items()\n",
    "}\n",
    "\n",
    "# layer 9 is the batch normalisation projection layer - Expanded_conv_project_BN\n",
    "optimal_thresholds_project = {\n",
    "    error_method: data[\"quantized_model\"].layers[9].activation_holder_quantizer.get_config()['threshold'][0] \n",
    "    for error_method, data in quantized_models_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XRAr8L5mvuLd",
   "metadata": {
    "id": "XRAr8L5mvuLd"
   },
   "source": [
    "### Distribution Plots\n",
    "\n",
    "These are the distributions of the two layers firstly, below relu and secondly Project_BN.\n",
    "\n",
    "The second distribution shows distinctly the difference in the result of the two error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VPb8tBNGpJjo",
   "metadata": {
    "id": "VPb8tBNGpJjo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_activations_relu, bins=100, alpha=0.5, label='Original')\n",
    "for method, threshold in optimal_thresholds_relu.items():\n",
    "    plt.axvline(threshold, linestyle='--', linewidth=2, label=f'{method}: {threshold:.2f}')\n",
    "\n",
    "plt.title('Activation Distribution with Optimal Quantization Thresholds First Relu Layer')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Df7eKzh4oj5X",
   "metadata": {
    "id": "Df7eKzh4oj5X"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_activations_project, bins=100, alpha=0.5, label='Original')\n",
    "for method, threshold in optimal_thresholds_project.items():\n",
    "    plt.axvline(threshold, linestyle='--', linewidth=2, label=f'{method}: {threshold:.2f}')\n",
    "\n",
    "plt.title('Activation Distribution with Optimal Quantization Thresholds Prohject BN layer')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c967d41-439d-405b-815f-be641f1768fe",
   "metadata": {
    "id": "4c967d41-439d-405b-815f-be641f1768fe"
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "Finally we can show the effect of these different thresholds on the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d9fd0-8005-4551-b853-3b52840639c2",
   "metadata": {
    "id": "092d9fd0-8005-4551-b853-3b52840639c2"
   },
   "outputs": [],
   "source": [
    "test_dataset_folder = './imagenet/val'\n",
    "evaluation_dataset = tutorial_tools.get_validation_dataset_fraction(0.005, test_dataset_folder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf7d04-7816-465c-9157-6068c0a4a08a",
   "metadata": {
    "id": "8ebf7d04-7816-465c-9157-6068c0a4a08a"
   },
   "outputs": [],
   "source": [
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "results = float_model.evaluate(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613",
   "metadata": {
    "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613"
   },
   "outputs": [],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "for error_method, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "\n",
    "    quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "\n",
    "    results = quantized_model.evaluate(evaluation_dataset, verbose=0)  # Set verbose=0 to suppress the log messages\n",
    "\n",
    "    evaluation_results[error_method] = results\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Results for {error_method}: Loss = {results[0]}, Accuracy = {results[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GpEZ2E1qzWl3",
   "metadata": {
    "id": "GpEZ2E1qzWl3"
   },
   "source": [
    "These results mirror the case for many models hence why MSE has been chosen by default by the MCT team.\n",
    "\n",
    "Each of MCT's error methods have a different effect on different models so it is always worth including this metric into hyper perameter tuning when trying to improve quantized model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {
    "id": "14877777"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "In this tutorial, we demonstrated the methods used to find a layers quantization threshold for activation. The process is similar for weight quantization but a representative dataset is not required. Use this code to assist with choosing error methods for your own model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c9b61-8056-4d06-8a2b-6e5fc56325f6",
   "metadata": {
    "id": "8c0c9b61-8056-4d06-8a2b-6e5fc56325f6"
   },
   "source": [
    "## Appendix\n",
    "\n",
    "Some code to assist with gaining information from each layer in the MCT quanisation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qml4LLmWZLP4",
   "metadata": {
    "id": "qml4LLmWZLP4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import inspect\n",
    "\n",
    "\n",
    "quantized_model = data[\"quantized_model\"]\n",
    "quantizer_object = quantized_model.layers[1]\n",
    "\n",
    "quantized_model = data[\"quantized_model\"]\n",
    "\n",
    "\n",
    "relu_layer_indices = []\n",
    "\n",
    "\n",
    "for i, layer in enumerate(quantized_model.layers):\n",
    "    # Convert the layer's configuration to a string\n",
    "    layer_config_str = str(layer.get_config())\n",
    "\n",
    "    layer_class_str = str(layer.__class__.__name__)\n",
    "\n",
    "    # Check if \"relu\" is mentioned in the layer's configuration or class name\n",
    "    if 'relu' in layer_config_str.lower() or 'relu' in layer_class_str.lower():\n",
    "        relu_layer_indices.append(i)\n",
    "\n",
    "print(\"Layer indices potentially using ReLU:\", relu_layer_indices)\n",
    "print(\"Number of relu layers \" + str(len(relu_layer_indices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207",
   "metadata": {
    "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207"
   },
   "outputs": [],
   "source": [
    "for error_method, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "    print(quantized_model.layers[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "\n",
    "\n",
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
