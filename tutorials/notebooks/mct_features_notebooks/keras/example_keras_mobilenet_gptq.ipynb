{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# Gradient-Based Post Training Quantization using the Model Compression Toolkit - A Quick-Start Guide\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_mobilenet_gptq.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates a pre-trained model quantization using the **Model Compression Toolkit (MCT)** with **Gradient-based PTQ (GPTQ)**. \n",
    "GPTQ stands as an optimization procedure that markedly enhances the performance of models undergoing post-training quantization.\n",
    "This is achieved through an optimization process applied post-quantization, specifically adjusting the rounding of quantized weights.\n",
    "GPTQ is especially effective in case of low bit width quantization and mixed precision quantization. \n",
    "\n",
    "This tutorial's scope is limited to demonstrating GPTQ usage. In this example, we quantize the model and evaluate the accuracy before and after quantization.\n",
    "\n",
    "For an example of a full quantization flow utilizing GPTQ see [full quantization tutorial](https://github.com/sony/model_optimization/blob/main/tutorials/notebooks/imx500_notebooks/keras/keras_yolov8n_for_imx500.ipynb)\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial we will cover:\n",
    "\n",
    "1. Gradient-Based Post-Training Quantization using MCT.\n",
    "2. Loading and preprocessing ImageNet's validation dataset.\n",
    "3. Constructing an unlabeled representative dataset.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install and import the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "tags": [],
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324"
   },
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "\n",
    "!pip install -q tensorflow=={TF_VER}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import importlib.util\n",
    "\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install -q model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c13aff20d208c51"
  },
  {
   "cell_type": "markdown",
   "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b",
   "metadata": {
    "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization and GPTQ optimization. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    \n",
    "    !cd imagenet && tar -xzf ILSVRC2012_devkit_t12.tar.gz && \\\n",
    "     mkdir ILSVRC2012_img_val && tar -xf ILSVRC2012_img_val.tar -C ILSVRC2012_img_val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c18b26e293b085e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rearrange the extracted data into folders per label "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6ac5021ed7a2ac3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "root = Path('./imagenet')\n",
    "imgs_dir = root / 'ILSVRC2012_img_val'\n",
    "target_dir = root /'val'\n",
    "\n",
    "def extract_labels():\n",
    "    !pip install -q scipy\n",
    "    import scipy\n",
    "    mat = scipy.io.loadmat(root / 'ILSVRC2012_devkit_t12/data/meta.mat', squeeze_me=True)\n",
    "    cls_to_nid = {s[0]: s[1] for i, s in enumerate(mat['synsets']) if s[4] == 0} \n",
    "    with open(root / 'ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt', 'r') as f:\n",
    "        return [cls_to_nid[int(cls)] for cls in f.readlines()]\n",
    "\n",
    "if not target_dir.exists():\n",
    "    labels = extract_labels()\n",
    "    for lbl in set(labels):\n",
    "        os.makedirs(target_dir / lbl)\n",
    "    \n",
    "    for img_file, lbl in zip(sorted(os.listdir(imgs_dir)), labels):\n",
    "        shutil.move(imgs_dir / img_file, target_dir / lbl)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9611251c15034c9"
  },
  {
   "cell_type": "markdown",
   "id": "028112db-3143-4fcb-96ae-e639e6476c31",
   "metadata": {
    "id": "028112db-3143-4fcb-96ae-e639e6476c31"
   },
   "source": [
    "### Representative Dataset\n",
    "\n",
    "GPTQ is a gradient-based optimization process, which requires representative dataset to perform inference and compute gradients. \n",
    "\n",
    "Separate representative datasets can be used for the PTQ statistics collection and for GPTQ. In this tutorial we use the same representative dataset for both.\n",
    "\n",
    "A complete pass through the representative dataset generator constitutes an epoch (batch_size x n_iter samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57",
   "metadata": {
    "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57"
   },
   "outputs": [],
   "source": [
    "def imagenet_preprocess_input(images, labels):\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408f624-ab68-4989-95f8-f9d327882840",
   "metadata": {
    "id": "0408f624-ab68-4989-95f8-f9d327882840"
   },
   "outputs": [],
   "source": [
    "def get_representative_dataset(n_iter=10, batch_size=50):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "\n",
    "    def representative_dataset():\n",
    "        for _ in range(n_iter):\n",
    "            yield [dataset.take(1).get_single_element()[0].numpy()]\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "representative_dataset_gen = get_representative_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
   },
   "source": [
    "## Model Gradient-Based Post-Training quantization using MCT\n",
    "\n",
    "This is the main part in which we quantize and our model.\n",
    "\n",
    "First, we load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cac59f-ec5e-41ca-b673-96220924a47c",
   "metadata": {
    "id": "80cac59f-ec5e-41ca-b673-96220924a47c"
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "float_model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9",
   "metadata": {
    "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9"
   },
   "source": [
    "Next, we create a GPTQ configuration with possible GPTQ optimization options (such as the number of epochs for the optimization process). MCT will quantize the model and start the GPTQ process to optimize the modelâ€™s parameters and quantization parameters.\n",
    "\n",
    "In addition, we need to define a TargetPlatformCapability object, representing the HW specifications on which we wish to eventually deploy our quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Create a GPTQ quantization configuration and set the number of training iterations. \n",
    "# 50 epochs are sufficient for this tutorial. For GPTQ run after mixed precision quantization a higher number of iterations\n",
    "# will be required.\n",
    "gptq_config = mct.gptq.get_keras_gptq_config(n_epochs=50)\n",
    "\n",
    "# Specify the target platform capability (TPC)\n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2edacb5b7779e4d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run model Gradient-based Post-Training Quantization\n",
    "Finally, we quantize our model using MCT's GPTQ API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6162dd6dd1fce7ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8373a-82a5-4b97-9a10-25ee2341d148",
   "metadata": {
    "id": "33f8373a-82a5-4b97-9a10-25ee2341d148"
   },
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.gptq.keras_gradient_post_training_quantization(\n",
    "    float_model,\n",
    "    representative_dataset_gen,\n",
    "    gptq_config=gptq_config,\n",
    "    target_platform_capabilities=tpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382ada6-d001-4564-907d-767fa4e9ec56",
   "metadata": {
    "id": "7382ada6-d001-4564-907d-767fa4e9ec56"
   },
   "source": [
    "That's it! Our model is now quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b",
   "metadata": {
    "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b"
   },
   "source": [
    "## Models evaluation\n",
    "\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, let's assume we downloaded the ImageNet validation dataset to a folder with the path below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c875-c4fc-4819-97e5-721805cba546",
   "metadata": {
    "tags": [],
    "id": "eef7c875-c4fc-4819-97e5-721805cba546"
   },
   "outputs": [],
   "source": [
    "def get_validation_dataset():\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=50,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=False,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "    return dataset\n",
    "\n",
    "evaluation_dataset = get_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889d217-90a6-4615-8569-38dc9cdd5999",
   "metadata": {
    "id": "9889d217-90a6-4615-8569-38dc9cdd5999"
   },
   "source": [
    "Let's start with the floating-point model evaluation.\n",
    "\n",
    "We need to compile the model before evaluation and set the loss and the evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a0ae9-beaa-4af8-8481-49d4917c2209",
   "metadata": {
    "id": "1d3a0ae9-beaa-4af8-8481-49d4917c2209"
   },
   "outputs": [],
   "source": [
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "results = float_model.evaluate(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4a6f3-86a0-4e6c-8229-a2ff514f7b8c",
   "metadata": {
    "id": "ead4a6f3-86a0-4e6c-8229-a2ff514f7b8c"
   },
   "source": [
    "Finally, let's evaluate the quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc377ee-39b4-4ced-95db-f7d51ab60848",
   "metadata": {
    "id": "1bc377ee-39b4-4ced-95db-f7d51ab60848"
   },
   "outputs": [],
   "source": [
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "results = quantized_model.evaluate(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e316c34cadd054e7"
  },
  {
   "cell_type": "markdown",
   "id": "ebfbb4de-5b6e-4732-83d3-a21e96cdd866",
   "metadata": {
    "id": "ebfbb4de-5b6e-4732-83d3-a21e96cdd866"
   },
   "source": [
    "You can see that we got a very small degradation with a compression rate of x4 !"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can export the model to Keras and TFLite:"
   ],
   "metadata": {
    "id": "6YjIdiRRjgkL"
   },
   "id": "6YjIdiRRjgkL"
  },
  {
   "cell_type": "code",
   "source": [
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path='qmodel.tflite',\n",
    "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE, quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)\n",
    "\n",
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path='qmodel.keras')\n"
   ],
   "metadata": {
    "id": "z3CA16-ojoFL"
   },
   "id": "z3CA16-ojoFL",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {
    "id": "14877777"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCT with gradient-based optimization with a few lines of code. We saw that we can achieve an x4 compression ratio with minimal performance degradation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "\n",
    "\n",
    "Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
