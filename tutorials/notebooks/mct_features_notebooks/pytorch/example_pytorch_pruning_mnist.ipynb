{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Pruning of a Fully-Connected PyTorch Model\n",
        "\n",
        "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_pruning_mnist.ipynb)\n",
        "\n",
        "Welcome to this tutorial, where we will guide you through the process of training, pruning, and retraining a fully connected neural network model using the PyTorch framework. The tutorial is organized in the following sections:\n",
        "1. We'll start by installing and importing the nessecry packages.\n",
        "2. Next, we will construct and train a simple neural network on the MNIST dataset.\n",
        "2. Following that, we'll introduce model pruning to reduce the model's size while maintaining accuracy.\n",
        "3. Finally, we'll retrain our pruned model to recover any performance lost due to pruning."
      ],
      "metadata": {
        "collapsed": false,
        "id": "81d379c3e030ceb3"
      },
      "id": "81d379c3e030ceb3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Pytorch and the Model Compression Toolkit\n",
        "We begin by setting up our environment by installing PyTorch and the Model Compression Toolkit, then importing them. These installations will allow us to define, train, prune, and retrain our neural network models within this notebook."
      ],
      "metadata": {
        "collapsed": false,
        "id": "5551cab7da5eb204"
      },
      "id": "5551cab7da5eb204"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision\n",
        "!pip install -q mct-nightly"
      ],
      "metadata": {
        "id": "6b36f0086537151b"
      },
      "id": "6b36f0086537151b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import model_compression_toolkit  as mct"
      ],
      "metadata": {
        "id": "c5ca27b4acf15197"
      },
      "id": "c5ca27b4acf15197"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Preprocessing MNIST Dataset\n",
        "Let's create a function to retrieve the train and test parts of the MNIST dataset, including preprocessing:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "c509bd917dbde9ef"
      },
      "id": "c509bd917dbde9ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# MNIST Data Loading and Preprocessing\n",
        "def load_and_preprocess_mnist(batch_size=128, root_path='./data'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "e2ebe94efb864812"
      },
      "id": "e2ebe94efb864812"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Fully-Connected Model\n",
        "In this section, we create a simple example of a fully connected model to demonstrate the pruning process. It consists of three linear layers with 128, 64, and 10 neurons."
      ],
      "metadata": {
        "collapsed": false,
        "id": "4c246b8487a151db"
      },
      "id": "4c246b8487a151db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Define the Fully-Connected Model\n",
        "class FCModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.fc_layers(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "9060e0fac2ae244"
      },
      "id": "9060e0fac2ae244"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Training Function\n",
        "\n",
        "Next, we'll define a function to train our neural network model. This function will handle the training loop, including forward propagation, loss calculation, backpropagation, and updating the model parameters. Additionally, we'll evaluate the model's performance on the validation dataset at the end of each epoch to monitor its accuracy."
      ],
      "metadata": {
        "collapsed": false,
        "id": "6acbc81a98082f80"
      },
      "id": "6acbc81a98082f80"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def test_model(model, test_loader):\n",
        "# Evaluate the model\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Training the Dense Model\n",
        "def train_model(model, train_loader, test_loader, device, epochs=6):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        accuracy = test_model(model, test_loader)\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Test Accuracy: {accuracy:.2f}%')\n",
        "    return model"
      ],
      "metadata": {
        "id": "86859a5f8b54f0c3"
      },
      "id": "86859a5f8b54f0c3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Dense Model\n",
        "We will now train the dense model using the MNIST dataset."
      ],
      "metadata": {
        "collapsed": false,
        "id": "1caf2d8a10673d90"
      },
      "id": "1caf2d8a10673d90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_loader, test_loader = load_and_preprocess_mnist()\n",
        "dense_model = FCModel()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dense_model = train_model(dense_model, train_loader, test_loader, device, epochs=6)"
      ],
      "metadata": {
        "id": "2d4660d484d4341b"
      },
      "id": "2d4660d484d4341b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Model Properties\n",
        "We will display our model's architecture, including layers, their types, and the number of parameters.\n",
        "Notably, MCT's structured pruning will target the first two dense layers for pruning, as these layers  have a higher number of channels compared to later layers, offering more opportunities for pruning without affecting accuracy significantly. This reduction can be effectively propagated by adjusting the input channels of subsequent layers."
      ],
      "metadata": {
        "collapsed": false,
        "id": "8af6f660db438605"
      },
      "id": "8af6f660db438605"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def display_model_params(model):\n",
        "    model_params = sum(p.numel() for p in model.state_dict().values())\n",
        "    for name, module in model.named_modules():\n",
        "        module_params = sum(p.numel() for p in module.state_dict().values())\n",
        "        if module_params > 0:\n",
        "            print(f'{name} number of parameters {module_params}')\n",
        "    print(f'{model}\\nTotal number of parameters {model_params}')\n",
        "    return model_params\n",
        "\n",
        "dense_model_params = display_model_params(dense_model)"
      ],
      "metadata": {
        "id": "b0741833e5af5c4f"
      },
      "id": "b0741833e5af5c4f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Representative Dataset\n",
        "We are creating a representative dataset to guide our model pruning process for computing importance score for each channel:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "9efc6fd59b15662"
      },
      "id": "9efc6fd59b15662"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a representative dataset\n",
        "ds_train_as_iter = iter(train_loader)\n",
        "\n",
        "def representative_data_gen() -> list:\n",
        "  yield [next(ds_train_as_iter)[0]]"
      ],
      "metadata": {
        "id": "f0e2bbdb3df563d3"
      },
      "id": "f0e2bbdb3df563d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning the Model\n",
        "Next,we'll proceed with pruning our trained model to decrease its size, targeting a 50% reduction in the memory footprint of the model's weights. Given that the model's weights utilize the float32 data type, where each parameter occupies 4 bytes, we calculate the memory requirement by multiplying the total number of parameters by 4."
      ],
      "metadata": {
        "collapsed": false,
        "id": "ac6c6db5635f8950"
      },
      "id": "ac6c6db5635f8950"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "compression_ratio = 0.5\n",
        "# Define Resource Utilization constraint for pruning. Each float32 parameter requires 4 bytes,\n",
        "# hence we multiply the total parameter count by 4 to calculate the memory footprint.\n",
        "target_resource_utilization = mct.core.ResourceUtilization(weights_memory=dense_model_params * 4 * compression_ratio)\n",
        "# Define a pruning configuration\n",
        "pruning_config=mct.pruning.PruningConfig(num_score_approximations=1)\n",
        "# Prune the model\n",
        "pruned_model, pruning_info = mct.pruning.pytorch_pruning_experimental(model=dense_model, target_resource_utilization=target_resource_utilization, representative_data_gen=representative_data_gen, pruning_config=pruning_config)"
      ],
      "metadata": {
        "id": "b524e66fbb96abd"
      },
      "id": "b524e66fbb96abd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model after pruning\n",
        "Let us view the model after the pruning operation and check the accuracy. We can see that pruning process caused a degradation in accuracy."
      ],
      "metadata": {
        "collapsed": false,
        "id": "9bf54933cd496543"
      },
      "id": "9bf54933cd496543"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "pruned_model_nparams = display_model_params(pruned_model)\n",
        "acc_before_retrain = test_model(pruned_model, test_loader)\n",
        "print(f'Pruned model accuracy before retraining {acc_before_retrain}%')"
      ],
      "metadata": {
        "id": "8c5cfe6b555acf63"
      },
      "id": "8c5cfe6b555acf63"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retraining the Pruned Model\n",
        "After pruning, we often need to retrain the model to recover any lost performance."
      ],
      "metadata": {
        "collapsed": false,
        "id": "a3eaaa9bb34ebf71"
      },
      "id": "a3eaaa9bb34ebf71"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "pruned_model_retrained = train_model(pruned_model, train_loader, test_loader, device, epochs=6)"
      ],
      "metadata": {
        "id": "9909464707e538a4"
      },
      "id": "9909464707e538a4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "In this tutorial, we demonstrated the process of training, pruning, and retraining a neural network model using the Model Compression Toolkit. We began by setting up our environment and loading the dataset, followed by building and training a fully connected neural network. We then introduced the concept of model pruning, specifically targeting the first two dense layers to efficiently reduce the model's memory footprint by 50%. After applying structured pruning, we evaluated the pruned model's performance and concluded the tutorial by fine-tuning the pruned model to recover any lost accuracy due to the pruning process. This tutorial provided a hands-on approach to model optimization through pruning, showcasing the balance between model size, performance, and efficiency."
      ],
      "metadata": {
        "collapsed": false,
        "id": "b5d01318e1c2c02d"
      },
      "id": "b5d01318e1c2c02d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "955d184da72b36c1"
      },
      "id": "955d184da72b36c1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
